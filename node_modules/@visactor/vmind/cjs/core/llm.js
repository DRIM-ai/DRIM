"use strict";

var __awaiter = this && this.__awaiter || function(thisArg, _arguments, P, generator) {
    return new (P || (P = Promise))((function(resolve, reject) {
        function fulfilled(value) {
            try {
                step(generator.next(value));
            } catch (e) {
                reject(e);
            }
        }
        function rejected(value) {
            try {
                step(generator.throw(value));
            } catch (e) {
                reject(e);
            }
        }
        function step(result) {
            var value;
            result.done ? resolve(result.value) : (value = result.value, value instanceof P ? value : new P((function(resolve) {
                resolve(value);
            }))).then(fulfilled, rejected);
        }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    }));
}, __importDefault = this && this.__importDefault || function(mod) {
    return mod && mod.__esModule ? mod : {
        default: mod
    };
};

Object.defineProperty(exports, "__esModule", {
    value: !0
}), exports.LLMManage = void 0;

const vutils_1 = require("@visactor/vutils"), axios_1 = __importDefault(require("axios")), llm_1 = require("../types/llm"), json_1 = require("../utils/json");

class LLMManage {
    constructor(options) {
        this.options = (0, vutils_1.merge)({}, this.getDefaultOptions(), options), this.historys = {};
    }
    getDefaultOptions() {
        return {
            url: "https://api.openai.com/v1/chat/completions",
            headers: {
                "Content-Type": "application/json"
            },
            method: "POST",
            model: llm_1.Model.GPT_4o,
            maxTokens: 2048,
            temperature: 0,
            frequencyPenalty: 0
        };
    }
    updateOptions(options) {
        this.options = (0, vutils_1.merge)({}, this.options, options);
    }
    run(name, messages, tools) {
        return __awaiter(this, void 0, void 0, (function*() {
            const {url: url = "", headers: headers, method: method, maxTokens: maxTokens, temperature: temperature, model: model, frequencyPenalty: frequency_penalty, topP: top_p, customRequestFunc: customRequestFunc} = this.options;
            this.historys[name] || (this.historys[name] = []);
            try {
                const res = (null == customRequestFunc ? void 0 : customRequestFunc[name]) && (0, 
                vutils_1.isFunction)(customRequestFunc[name]) ? yield customRequestFunc[name](messages, tools, this.options) : yield (0, 
                axios_1.default)(url, {
                    method: method,
                    headers: headers,
                    data: {
                        model: model,
                        messages: messages,
                        tools: tools,
                        max_tokens: maxTokens,
                        temperature: temperature,
                        stream: !1,
                        frequency_penalty: frequency_penalty,
                        top_p: top_p
                    }
                }).then((response => response.data)), {logId: logId, id: id} = res;
                return this.historys[name].push({
                    logId: logId,
                    id: id
                }), res.error ? {
                    error: res.error
                } : res;
            } catch (err) {
                return {
                    error: err
                };
            }
        }));
    }
    parseTools(res) {
        var _a;
        const {choices: choices = [], error: error} = res;
        if (error) return {
            error: error
        };
        if (!choices.length) return {
            error: "llm response is empty"
        };
        try {
            return ((null === (_a = choices[0].message) || void 0 === _a ? void 0 : _a.tool_calls) || []).map((toolCall => Object.assign(Object.assign({}, toolCall), {
                function: Object.assign(Object.assign({}, toolCall.function), {
                    arguments: (0, json_1.parseLLMJson)(toolCall.function.arguments)
                })
            })));
        } catch (err) {
            return {
                error: err
            };
        }
    }
    parseJson(res) {
        var _a;
        const {choices: choices = [], error: error} = res;
        if (error) return {
            error: error
        };
        if (!choices.length) return {
            error: "llm response is empty"
        };
        try {
            const content = choices[0].message.content;
            if ("tool_calls" === (null === (_a = choices[0]) || void 0 === _a ? void 0 : _a.finish_reason)) return {};
            const jsonStr = (0, json_1.revisedJSONStr)((0, json_1.matchJSONStr)(content));
            return (0, json_1.parseLLMJson)(jsonStr, "```");
        } catch (err) {
            return {
                error: err
            };
        }
    }
}

exports.LLMManage = LLMManage;
//# sourceMappingURL=llm.js.map